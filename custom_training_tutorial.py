# -*- coding: utf-8 -*-
"""Custom_training_tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QHxK0c7c2BcmviRPgYww0OWcMlaZ9f-5

Tutorial to categorize penguins by species by:


*  Importing dataset
*  Building a linear model
*  Training the model
*  Evaluate model
*  nake predictions

# Penguin classification

Classification based on: body weight, flipper length, beaks and culmen data. Three species of the 18 in the dataset will be classified.

# Setup
install tfds-nightly and restart runtime afterwards
"""

pip install -q tfds-nightly

import os
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

print("TensorFlow version: {}".format(tf.__version__))
print("TensorFlow Datasets version: ",tfds.__version__)

"""# Import the dataset
penguins/processed is already cleaned, normalized etc

### Preview data
inspect first 5 records
"""

ds_preview, info = tfds.load('penguins/simple', split='train', with_info=True)
df = tfds.as_dataframe(ds_preview.take(5), info)
print(df)
print(info.features)

"""each line is one example, where the first six fields are features and the last coclumn is the label. Here, the name of the species is represented by a number

create a list with the names
"""

class_names = ['Ad√©lie', 'Chinstrap', 'Gentoo']

"""### Download preprocessed dataset

use tfds.load to download preprocessed dataset.
"""

ds_split, info = tfds.load("penguins/processed", split=['train[:20%]', 'train[20%:]'], as_supervised=True, with_info=True)

ds_test = ds_split[0]
ds_train = ds_split[1]
assert isinstance(ds_test, tf.data.Dataset)

print(info.features)
df_test = tfds.as_dataframe(ds_test.take(5), info)
print("Test dataset sample: ")
print(df_test)

df_train = tfds.as_dataframe(ds_train.take(5), info)
print("Train dataset sample: ")
print(df_train)

ds_train_batch = ds_train.batch(32)

features, labels = next(iter(ds_train_batch))

print(features)
print(labels)

plt.scatter(features[:,0],
            features[:,2],
            c=labels,
            cmap='viridis')

plt.xlabel("Body Mass")
plt.ylabel("Culmen Length")
plt.show()

"""# Build a linear model:

The keras sqeuential model regires the input shape as parameter of the input features and has an output layer with 3 nodes containing the corresponding probabilities. 
"""

model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(3)
])

"""The activation function determines the output shape of each node and contain nonlinearities.

# use the model

quick look at the model:
"""

predictions = model(features)
predictions[:5]

"""convert digits to probability with softmax:

"""

tf.nn.softmax(predictions[:5])

"""argmax to get predicted class index"""

print("Prediction: {}".format(tf.math.argmax(predictions, axis=1)))
print("    Labels: {}".format(labels))

"""# Train the model:
While training, the model gets gradually optimized. caution: overfitting: model learns all data and cannot adapt to new data.

### Define loss and gradent fcn
the loss measures the accuracy of the model
"""

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

def loss(model, x, y, training):
  # training=training is needed only if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  y_ = model(x, training=training)

  return loss_object(y_true=y, y_pred=y_)

l = loss(model, features, labels, training=False)
print("Loss test: {}".format(l))

"""GradientTape to optmize model"""

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets, training=True)
  return loss_value, tape.gradient(loss_value, model.trainable_variables)

"""### Create optimizer
optimizers apply the gradients to minimize loss fcn.
With SGD algo used here. Learning_rate sets step size of learning
"""

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

"""single optimization step:"""

loss_value, grads = grad(model, features, labels)

print("Step: {}, Initial Loss: {}".format(optimizer.iterations.numpy(),
                                          loss_value.numpy()))

optimizer.apply_gradients(zip(grads, model.trainable_variables))

print("Step: {},         Loss: {}".format(optimizer.iterations.numpy(),
                                          loss(model, features, labels, training=True).numpy()))

"""### Training Loop
"
*    Iterate each epoch. An epoch is one pass through the dataset.
*    Within an epoch, iterate over each example in the training Dataset grabbing its features (x) and label (y).
*    Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.
*    Use an optimizer to update the model's parameters.
*    Keep track of some stats for visualization.
*    Repeat for each epoch.
"
"""

## Note: Rerunning this cell uses the same model parameters

# Keep results for plotting
train_loss_results = []
train_accuracy_results = []

num_epochs = 201

for epoch in range(num_epochs):
  epoch_loss_avg = tf.keras.metrics.Mean()
  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

  # Training loop - using batches of 32
  for x, y in ds_train_batch:
    # Optimize the model
    loss_value, grads = grad(model, x, y)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    # Track progress
    epoch_loss_avg.update_state(loss_value)  # Add current batch loss
    # Compare predicted label to actual label
    # training=True is needed only if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    epoch_accuracy.update_state(y, model(x, training=True))

  # End epoch
  train_loss_results.append(epoch_loss_avg.result())
  train_accuracy_results.append(epoch_accuracy.result())

  if epoch % 50 == 0:
    print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))

"""Alternatively, you could use the built-in Keras Model.fit(ds_train_batch) method to train your model. 

### Visualization
"""

fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))
fig.suptitle('Training Metrics')

axes[0].set_ylabel("Loss", fontsize=14)
axes[0].plot(train_loss_results)

axes[1].set_ylabel("Accuracy", fontsize=14)
axes[1].set_xlabel("Epoch", fontsize=14)
axes[1].plot(train_accuracy_results)
plt.show()

"""# evaluate effectiveness
### set up test set
in evaluation, the examples come from differend test set than the training one. for evaluation, use the ds_test_batch

### evaluate model on test dataset
only one epoch needed
"""

test_accuracy = tf.keras.metrics.Accuracy()
ds_test_batch = ds_test.batch(10)

for (x, y) in ds_test_batch:
  # training=False is needed only if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  logits = model(x, training=False)
  prediction = tf.math.argmax(logits, axis=1, output_type=tf.int64)
  test_accuracy(prediction, y)

print("Test set accuracy: {:.3%}".format(test_accuracy.result()))

"""You can also use the model.evaluate(ds_test, return_dict=True) keras function to get accuracy information on your test dataset. """

tf.stack([y,prediction],axis=1)

"""### Use model to make predictions:
use the trained (fairly good) model to predict species
"""

predict_dataset = tf.convert_to_tensor([
    [0.3, 0.8, 0.4, 0.5,],
    [0.4, 0.1, 0.8, 0.5,],
    [0.7, 0.9, 0.8, 0.4]
])

# training=False is needed only if there are layers with different
# behavior during training versus inference (e.g. Dropout).
predictions = model(predict_dataset, training=False)

for i, logits in enumerate(predictions):
  class_idx = tf.math.argmax(logits).numpy()
  p = tf.nn.softmax(logits)[class_idx]
  name = class_names[class_idx]
  print("Example {} prediction: {} ({:4.1f}%)".format(i, name, 100*p))