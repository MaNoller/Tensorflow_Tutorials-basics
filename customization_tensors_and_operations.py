# -*- coding: utf-8 -*-
"""Customization_Tensors_and_operations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_GqB8szcRsh2MDMLInabAs7cUtrhXtZm
"""

import tensorflow as tf

"""# Tensors

Tensors are multi-dimensional arrays. Like ndarrays, tf.Tensor have a data type and shape.
"""

print(tf.math.add(1, 2))
print(tf.math.add([1, 2], [3, 4]))
print(tf.math.square(5))
print(tf.math.reduce_sum([1, 2, 3]))

# Operator overloading is also supported
print(tf.math.square(2) + tf.math.square(3))

x = tf.linalg.matmul([[1]], [[2, 3]])
print(x)
print(x.shape)
print(x.dtype)

"""unlike ndarrays, Tensors are immutable

### Numpy compatibility

Conversion between TensorFlow and NumPy is easy:


*   TF automatically convert numpy arrays to tensors
*   Numpy automatically convert Tensors to ndarrays

to explicitly convert tensors to ndarrays. use .numpy(). 

"""

import numpy as np

ndarray = np.ones([3, 3])

print("TensorFlow operations convert numpy arrays to Tensors automatically")
tensor = tf.math.multiply(ndarray, 42)
print(tensor)


print("And NumPy operations convert Tensors to NumPy arrays automatically")
print(np.add(tensor, 1))

print("The .numpy() method explicitly converts a Tensor to a numpy array")
print(tensor.numpy())

"""# GPU acceleration

TF operations can be accelerated using the GPU. TF automatically decides whether to use GPU or CPU for an operation. 
"""

x = tf.random.uniform([3, 3])

print("Is there a GPU available: "),
print(tf.config.list_physical_devices("GPU"))

print("Is the Tensor on GPU #0:  "),
print(x.device.endswith('GPU:0'))

"""### Device Names

Tensor.device provides a fully qualified string name of the hosting device. It encodes identifier for networkadress and device within host. This is required for distributed execution.

### Explicit device placement
placement in TF refers to how operations are assigned a device for execution. When not explicitly guided, tf automatically assigns if needed. Manually, use tf.device:
"""

import time

def time_matmul(x):
  start = time.time()
  for loop in range(10):
    tf.linalg.matmul(x, x)

  result = time.time()-start

  print("10 loops: {:0.2f}ms".format(1000*result))

# Force execution on CPU
print("On CPU:")
with tf.device("CPU:0"):
  x = tf.random.uniform([1000, 1000])
  assert x.device.endswith("CPU:0")
  time_matmul(x)

# Force execution on GPU #0 if available
if tf.config.list_physical_devices("GPU"):
  print("On GPU:")
  with tf.device("GPU:0"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.
    x = tf.random.uniform([1000, 1000])
    assert x.device.endswith("GPU:0")
    time_matmul(x)

"""# Datasets
here, a pipeline will be build to feed data to the model

### Create a source dataset

Create a source dataset using one of the factory functions like tf.data.Dataset.from_tensors, tf.data.Dataset.from_tensor_slices, or using objects that read from files like tf.data.TextLineDataset or tf.data.TFRecordDataset. Refer to the Reading input data section of the tf.data: Build TensorFlow input pipelines guide for more information.
"""

ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])

# Create a CSV file
import tempfile
_, filename = tempfile.mkstemp()

with open(filename, 'w') as f:
  f.write("""Line 1
Line 2
Line 3
  """)

ds_file = tf.data.TextLineDataset(filename)

"""### Apply transformations"""

ds_tensors = ds_tensors.map(tf.math.square).shuffle(2).batch(2)

ds_file = ds_file.batch(2)

"""### iterate"""

print('Elements of ds_tensors:')
for x in ds_tensors:
  print(x)

print('\nElements in ds_file:')
for x in ds_file:
  print(x)