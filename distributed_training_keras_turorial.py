# -*- coding: utf-8 -*-
"""Distributed_training_keras_turorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yuRDPEt3LoNfwBWT4vD8YX3nebq0cyBO

With tf.distribute.Strategy, it is possible to distribute training across multiple processing units, even with existing models requiring only minimal changes.

Here, it will be used to perform training repliccation with synchronous training on multiple gpus on one machione. it copies all variables  to each processor and, using all-reduce, combines the gradients of all processors for computation.

with keras the model will be build and fit.

# Setup
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow_datasets as tfds
import tensorflow as tf

import os

# Load the TensorBoard notebook extension.
# %load_ext tensorboard

"""# Download Dataset

The MNIST dataset will be used. with_info= True includes metadata.
"""

datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)

mnist_train, mnist_test = datasets['train'], datasets['test']

"""# Define Distribution Strategy
The MirroredStrategy obj will handle distribution and provide context manager for including the model
"""

strategy = tf.distribute.MirroredStrategy()

print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

"""# Setup input pipeline
When using multiuple gpus, the batch size can be increased.

best practice: use largest batch size that fits gpu and tune learning rate accordingly
"""

# You can also do info.splits.total_num_examples to get the total
# number of examples in the dataset.

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

"""normalize pixel values from 0-255 to 0-1"""

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255

  return image, label

"""Apply to train and test ds. furthermore: batch, shuffle and cache:"""

checktrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

"""# Create Model

create and compile model in strategy.scope
"""

with strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])

  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

"""# Define callbacks
define the following callbacks:


*   Tensorboard to write logs, allowing visualization
*   ModelCheckpoint, saves model with certain frequency
*   BackupAndRestore, for fault tolerance, backs up the model and epoch number
*   LearningrateScheduler, changes learning rate variably

"""

# Define the checkpoint directory to store the checkpoints.
checkpoint_dir = './training_checkpoints'
# Define the name of the checkpoint files.
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

# Define a function for decaying the learning rate.
# You can define any decay function you need.
def decay(epoch):
  if epoch < 3:
    return 1e-3
  elif epoch >= 3 and epoch < 7:
    return 1e-4
  else:
    return 1e-5

# Define a callback for printing the learning rate at the end of each epoch.
class PrintLR(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(        epoch + 1, model.optimizer.lr.numpy()))

# Put all the callbacks together.
callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    tf.keras.callbacks.LearningRateScheduler(decay),
    PrintLR()
]

"""# Train and evaluate

Same as usual
"""

EPOCHS = 12

model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)

"""check saved checkpoints"""

# Check the checkpoint directory.
ls {checkpoint_dir}

"""check accuracy"""

model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))

eval_loss, eval_acc = model.evaluate(eval_dataset)

print('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=logs

ls -sh ./logs

"""export model"""

path = 'saved_model/'

model.save(path, save_format='tf')

"""load without scope"""

unreplicated_model = tf.keras.models.load_model(path)

unreplicated_model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=['accuracy'])

eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)

print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))

"""load with scope"""

with strategy.scope():
  replicated_model = tf.keras.models.load_model(path)
  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                           optimizer=tf.keras.optimizers.Adam(),
                           metrics=['accuracy'])

  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)
  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))