# -*- coding: utf-8 -*-
"""Load_CSV_Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aIELyWLyFeoXUDx7gkHpTHE5-WAzzwsx
"""

import pandas as pd
import numpy as np

# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf
from tensorflow.keras import layers

"""# In memory data
The simplest way is to load the data into memory as pd df or np array

Download into pd Df:
"""

abalone_train = pd.read_csv(
    "https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv",
    names=["Length", "Diameter", "Height", "Whole weight", "Shucked weight",
           "Viscera weight", "Shell weight", "Age"])

abalone_train.head()

"""Task: predict age from other measurments"""

abalone_features = abalone_train.copy()
abalone_labels = abalone_features.pop('Age')

abalone_features = np.array(abalone_features)
abalone_features

"""Regression model to prredict age. Since there is only one input tensor, Sequential is sufficient"""

abalone_model = tf.keras.Sequential([
  layers.Dense(64),
  layers.Dense(1)
])

abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),
                      optimizer = tf.keras.optimizers.Adam())

abalone_model.fit(abalone_features, abalone_labels, epochs=10)

"""next, add preprocessing

# Preprocessing
Normalization is good practice, convenient way to do so is keras.layers.Normalization
"""

normalize = layers.Normalization()

"""adapt layer to data:"""

normalize.adapt(abalone_features)

norm_abalone_model = tf.keras.Sequential([
  normalize,
  layers.Dense(64),
  layers.Dense(1)
])

norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),
                           optimizer = tf.keras.optimizers.Adam())

norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)

"""# Mixed Data Types

Titanic dataset contains passenger information, predict who survives.

Load as pd df
"""

titanic = pd.read_csv("https://storage.googleapis.com/tf-datasets/titanic/train.csv")
titanic.head()

titanic_features = titanic.copy()
titanic_labels = titanic_features.pop('survived')

"""Here, the datatypes vary and can not just be used in Sequential model.

One possibility is to convert categorical into numerical data. Here the keras functional api will be used. It works with symbolic tensors.

"Normal "eager" tensors have a value. In contrast these "symbolic" tensors do not. Instead they keep track of which operations are run on them, and build a representation of the calculation, that you can run later. Here's a quick example:"
"""

# Create a symbolic input
input = tf.keras.Input(shape=(), dtype=tf.float32)

# Perform a calculation using the input
result = 2*input + 1

# the result doesn't have a value
result

calc = tf.keras.Model(inputs=input, outputs=result)

print(calc(1).numpy())
print(calc(2).numpy())

"""For preprocessing, a set of symbolic keras.input obj, matching the datatypes and names of the csv will be build"""

inputs = {}

for name, column in titanic_features.items():
  dtype = column.dtype
  if dtype == object:
    dtype = tf.string
  else:
    dtype = tf.float32

  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)

inputs

"""Concatenate numeric inputs and normalize them"""

numeric_inputs = {name:input for name,input in inputs.items()
                  if input.dtype==tf.float32}

x = layers.Concatenate()(list(numeric_inputs.values()))
norm = layers.Normalization()
norm.adapt(np.array(titanic[numeric_inputs.keys()]))
all_numeric_inputs = norm(x)

all_numeric_inputs

"""collect symbolic preproccess results"""

preprocessed_inputs = [all_numeric_inputs]

"""For strings, tf.keras.layers.StringLookup is used to map from strings to integer indices in a cvocab. Then tf.keras.layers.CategoryEncoding  to convert indexes to float32"""

for name, input in inputs.items():
  if input.dtype == tf.float32:
    continue

  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))
  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())

  x = lookup(input)
  x = one_hot(x)
  preprocessed_inputs.append(x)

"""all together"""

preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)

titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)

tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir="LR", dpi=72, show_shapes=True)

"""In thios model, just the input processing is contained. keras models dont automatically convert to pd df, because it is not clear if it should be one tensor or a dict of tensors. Convert to dict of tensors"""

titanic_features_dict = {name: np.array(value) 
                         for name, value in titanic_features.items()}

"""Slice out the first training example and pass it to this preprocessing model, you see the numeric features and string one-hots all concatenated together:"""

features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}
titanic_preprocessing(features_dict)

"""model"""

def titanic_model(preprocessing_head, inputs):
  body = tf.keras.Sequential([
    layers.Dense(64),
    layers.Dense(1)
  ])

  preprocessed_inputs = preprocessing_head(inputs)
  result = body(preprocessed_inputs)
  model = tf.keras.Model(inputs, result)

  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.Adam())
  return model

titanic_model = titanic_model(titanic_preprocessing, inputs)

"""When you train the model, pass the dictionary of features as x, and the label as y."""

titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)

titanic_model.save('test')
reloaded = tf.keras.models.load_model('test')

features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}

before = titanic_model(features_dict)
after = reloaded(features_dict)
assert (before-after)<1e-3
print(before)
print(after)

"""# Using tf.data

in the last section, shuffling and batching was automatic. If more control is desired, use tf.data

### On in memory data
manually slice up dict of features from prev section
"""

import itertools

def slices(features):
  for i in itertools.count():
    # For each feature take index `i`
    example = {name:values[i] for name, values in features.items()}
    yield example

for example in slices(titanic_features_dict):
  for name, value in example.items():
    print(f"{name:19s}: {value}")
  break

"""most basic loader is Dataset.from_tensor_slices. this returns a tf.data.Dataset"""

features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict)

for example in features_ds:
  for name, value in example.items():
    print(f"{name:19s}: {value}")
  break

"""from_tensor_slices can handle nested structs. the following makes pairs (feature_dict, labels)"""

titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))

"""before training shuffling and batching:"""

titanic_batches = titanic_ds.shuffle(len(titanic_labels)).batch(32)

"""pass dataset instead of features and labels to model.fit"""

titanic_model.fit(titanic_batches, epochs=5)

"""### From single file
so far: working with in-memory data. tf.data is scalable for building pipelines:
"""

titanic_file_path = tf.keras.utils.get_file("train.csv", "https://storage.googleapis.com/tf-datasets/titanic/train.csv")

"""create dataset:"""

titanic_csv_ds = tf.data.experimental.make_csv_dataset(
    titanic_file_path,
    batch_size=5, # Artificially small to make examples easier to show.
    label_name='survived',
    num_epochs=1,
    ignore_errors=True,)

"""for convenience: column headers can be used as dict keys, column type autodetermination"""

for batch, label in titanic_csv_ds.take(1):
  for key, value in batch.items():
    print(f"{key:20s}: {value}")
  print()
  print(f"{'label':20s}: {label}")

traffic_volume_csv_gz = tf.keras.utils.get_file(
    'Metro_Interstate_Traffic_Volume.csv.gz', 
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz",
    cache_dir='.', cache_subdir='traffic')

"""set compression_type to read from compressed file"""

traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(
    traffic_volume_csv_gz,
    batch_size=256,
    label_name='traffic_volume',
    num_epochs=1,
    compression_type="GZIP")

for batch, label in traffic_volume_csv_gz_ds.take(1):
  for key, value in batch.items():
    print(f"{key:20s}: {value[:5]}")
  print()
  print(f"{'label':20s}: {label[:5]}")

"""### caching

it may help performance to use caching so the csv is only parsed on first epoch
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for i, (batch, label) in enumerate(traffic_volume_csv_gz_ds.repeat(20)):
#   if i % 40 == 0:
#     print('.', end='')
# print()
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# caching = traffic_volume_csv_gz_ds.cache().shuffle(1000)
# 
# for i, (batch, label) in enumerate(caching.shuffle(1000).repeat(20)):
#   if i % 40 == 0:
#     print('.', end='')
# print()
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# snapshot = tf.data.experimental.snapshot('titanic.tfsnap')
# snapshotting = traffic_volume_csv_gz_ds.apply(snapshot).shuffle(1000)
# 
# for i, (batch, label) in enumerate(snapshotting.shuffle(1000).repeat(20)):
#   if i % 40 == 0:
#     print('.', end='')
# print()
#

"""Tutorial parts containing info to: multiple files and lower level functions will not be included here, for more info: https://www.tensorflow.org/tutorials/load_data/csv#multiple_files"""