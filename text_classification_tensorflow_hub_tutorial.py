# -*- coding: utf-8 -*-
"""Text_Classification_TensorFlow_Hub_Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BwP1GDjOGJdEOFTKkRe5SM2s918AFHKq

Sentiment analysis using imdb database with 1:1 training:testing split USING TensorFlow HUB
"""

!pip install tensorflow-hub
!pip install tensorflow-datasets

import os
import numpy as np

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

print("Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.list_physical_devices("GPU") else "NOT AVAILABLE")

# Split the training set into 60% and 40% to end up with 15,000 examples
# for training, 10,000 examples for validation and 25,000 examples for testing.
train_data, validation_data, test_data = tfds.load(
    name="imdb_reviews", 
    split=('train[:60%]', 'train[60%:]', 'test'),
    as_supervised=True)

"""# Explore data"""

train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))
train_examples_batch

train_labels_batch

"""# Build the Model

For this example a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2 is used
"""

embedding = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=True)
hub_layer(train_examples_batch[:3])

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.summary()

"""TF Hub layer: uses the pre-trained model to map sentence to its embedding vector. Splits sentence into tokens and embeds these while combining the embedding. embedding_dimension=50

# Loss Fcn and Optimizer

Binary-crossentropy: good for handling probabilities:  measures the "distance" between probability distributions
"""

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""# Train the Model"""

history = model.fit(train_data.shuffle(10000).batch(512),
                    epochs=10,
                    validation_data=validation_data.batch(512),
                    verbose=1)

"""# Evaluation"""

results = model.evaluate(test_data.batch(512), verbose=2)

for name, value in zip(model.metrics_names, results):
  print("%s: %.3f" % (name, value))