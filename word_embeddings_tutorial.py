# -*- coding: utf-8 -*-
"""word_embeddings_tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m1DJBEdro4wY3NHxNaO3qVWU8O5ggBLh

#representing text as numbers
models can take vectors as input. when working with text, the strings have to be converted to numbers (=vectorized)

##One-hot encoding
the first approach is one-hot encoding. Idea: create a 0-vector with the length equal to vocabulary to represent each word. When the word is present, a 1 is in the vector instead of 0. To represent the encoding of a sentence, one could concatenate the vectors

*Key Point: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.*

##Encode each word with a unique number
give each word a unique number to get a dense vector. Downsides: arbitrary encoding (no relationships between words), can be challenging for model to interpret.

##Word embeddings
give a dense representation wimilar to encoding. no need to specify encoding by hand! Embedding is a dense vector w floating point values.instead of manually encoding, the values are trainable parameters.

#Setup
"""

import io
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers import TextVectorization

"""#Download dataset
Large movie review dataset will be used for sentiment analysis model
"""

url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file("aclImdb_v1.tar.gz", url,
                                  untar=True, cache_dir='.',
                                  cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
os.listdir(dataset_dir)

"""have a look at data dir"""

train_dir = os.path.join(dataset_dir, 'train')
os.listdir(train_dir)

"""pos and neg reviews will be used."""

remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)

batch_size = 1024
seed = 123
train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', batch_size=batch_size, validation_split=0.2,
    subset='training', seed=seed)
val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', batch_size=batch_size, validation_split=0.2,
    subset='validation', seed=seed)

"""examples:"""

for text_batch, label_batch in train_ds.take(1):
  for i in range(5):
    print(label_batch[i].numpy(), text_batch.numpy()[i])

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""#Embedding layer
using keras embedding layer:
can be interpreted as a lookup table that maps int vals (repr. words) to dense vectors. the dimensionality can be varied
"""

# Embed a 1,000 word vocabulary into 5 dimensions.
embedding_layer = tf.keras.layers.Embedding(1000, 5)

"""WHen creating a layer, weights are randomly intis. they will be adjustes while training.

WHEN PASSING AN INT TO AN EMBEDDING LAYER, RESULT REPLACES INT WITH VECTOR FROM EMBEDDING TABLE
"""

result = embedding_layer(tf.constant([1, 2, 3]))
result.numpy()

"""For text, embedding layer takes 2d tensors of int (shape (samples,sequence_length)) where each entry is a sequence of ints.  

Returned tensor has one more acis than input, along which the embedding vectors are aligned along.
"""

result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))
result.shape

"""#Text preprocessing
Set a textVectorization layer 
"""

# Create a custom standardization function to strip HTML break tags '<br />'.
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation), '')


# Vocabulary size and number of words in a sequence.
vocab_size = 10000
sequence_length = 100

# Use the text vectorization layer to normalize, split, and map strings to
# integers. Note that the layer uses the custom standardization defined above.
# Set maximum_sequence length as all samples are not of the same length.
vectorize_layer = TextVectorization(
    standardize=custom_standardization,
    max_tokens=vocab_size,
    output_mode='int',
    output_sequence_length=sequence_length)

# Make a text-only dataset (no labels) and call adapt to build the vocabulary.
text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)

"""#Create classification model
bag of words-style:
*  Tectvectorization converts string to voc indices. already initialized
*  Embedding takes integer coded voc and looks up embedding vector vor each word-index. are learned by training, resulting dimensions: (batch, sequence, embedding)
*  GlobaÄºAveragePooling averages over sequence dimension, allows for variable input length
*  output is piped through dense layer w 16 units
* dense output layer at last
"""

embedding_dim=16

model = Sequential([
  vectorize_layer,
  Embedding(vocab_size, embedding_dim, name="embedding"),
  GlobalAveragePooling1D(),
  Dense(16, activation='relu'),
  Dense(1)
])

"""#Compile and train"""

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=15,
    callbacks=[tensorboard_callback])

model.summary()



# Commented out IPython magic to ensure Python compatibility.
#docs_infra: no_execute
# %load_ext tensorboard
# %tensorboard --logdir logs

"""#save embeddings
since embeddings are weights of the mebedding layer, we want to save them. shape: (vocab_size, embedding_dimension)
"""

weights = model.get_layer('embedding').get_weights()[0]
vocab = vectorize_layer.get_vocabulary()

out_v = io.open('vectors.tsv', 'w', encoding='utf-8')
out_m = io.open('metadata.tsv', 'w', encoding='utf-8')

for index, word in enumerate(vocab):
  if index == 0:
    continue  # skip 0, it's padding.
  vec = weights[index]
  out_v.write('\t'.join([str(x) for x in vec]) + "\n")
  out_m.write(word + "\n")
out_v.close()
out_m.close()

try:
  from google.colab import files
  files.download('vectors.tsv')
  files.download('metadata.tsv')
except Exception:
  pass